{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1d80e21",
   "metadata": {},
   "source": [
    "Day1-7~8교시: MLflow 기반 실험 추적\n",
    "- run 생성, param/metric 기록, 모델 artifact 저장, 실험 비교\n",
    "- 산출물: MLflow run 캡처, 실험 비교표 (templates/mlflow_run_capture_template.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a5af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "BASE = \"/content\" if IN_COLAB else os.getcwd()\n",
    "CSV_PATH = os.path.join(BASE, \"TestData\", \"Social_Network_Ads.csv\")\n",
    "MLFLOW_DIR = os.path.join(BASE, \"mlruns\")\n",
    "SEED = 42\n",
    "\n",
    "mlflow.set_tracking_uri(\"file://\" + os.path.abspath(MLFLOW_DIR))\n",
    "mlflow.set_experiment(\"ncs_spark_day1\")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Day1_MLflow\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53de36e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(CSV_PATH)\n",
    "indexer = StringIndexer(inputCol=\"Gender\", outputCol=\"Gender_idx\").setHandleInvalid(\"keep\")\n",
    "encoder = OneHotEncoder(inputCols=[\"Gender_idx\"], outputCols=[\"Gender_ohe\"])\n",
    "assembler = VectorAssembler(inputCols=[\"Age\", \"EstimatedSalary\", \"Gender_ohe\"], outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "pipeline = Pipeline(stages=[indexer, encoder, assembler, scaler])\n",
    "df_ready = pipeline.fit(df).transform(df)\n",
    "data = df_ready.select(\"scaled_features\", \"Purchased\").withColumnRenamed(\"scaled_features\", \"features\")\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548842f",
   "metadata": {},
   "source": [
    "Run 1: Baseline LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612a84ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"baseline_lr\"):\n",
    "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"Purchased\")\n",
    "    model = lr.fit(train_data)\n",
    "    preds = model.transform(test_data)\n",
    "    auc = BinaryClassificationEvaluator(labelCol=\"Purchased\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\").evaluate(preds)\n",
    "    mlflow.log_param(\"model\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"regParam\", str(lr.getRegParam()))\n",
    "    mlflow.log_metric(\"test_auc\", auc)\n",
    "    mlflow.spark.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7cdbd",
   "metadata": {},
   "source": [
    "Run 2: Tuned LR (CrossValidator best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9ec285",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"Purchased\")\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Purchased\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "param_grid = ParamGridBuilder().addGrid(lr.regParam, [0.01, 0.1]).addGrid(lr.elasticNetParam, [0.0, 0.5]).build()\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3, seed=SEED)\n",
    "\n",
    "with mlflow.start_run(run_name=\"tuned_lr\"):\n",
    "    cv_model = cv.fit(train_data)\n",
    "    best = cv_model.bestModel\n",
    "    preds = cv_model.transform(test_data)\n",
    "    auc_tuned = evaluator.evaluate(preds)\n",
    "    mlflow.log_param(\"model\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"regParam\", str(best.getRegParam()))\n",
    "    mlflow.log_param(\"elasticNetParam\", str(best.getElasticNetParam()))\n",
    "    mlflow.log_metric(\"test_auc\", auc_tuned)\n",
    "    mlflow.spark.log_model(best, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f674e5",
   "metadata": {},
   "source": [
    "실험 비교: MLflow UI에서 mlruns 폴더를 tracking_uri로 열어 run 목록 확인. 산출물 템플릿에 run_id, metric 기록."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df814f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLflow runs saved under:\", MLFLOW_DIR)\n",
    "print(\"로컬에서 확인: mlflow ui --backend-store-uri\", MLFLOW_DIR)\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
